# Behavioral cloning:
This approach just uses a dataset I generated by playing with friends to train a model to play the move it thinks a human would play.

## Results:
Just training a policy network on the augmented dataset gave suprisingly good results (it even beat me once). I couldn't succesfully use the REINFORCE algorithm to improve the policy likely due to lack of noise during self-play. In the beginning stages of the game, the algorithm has a good recollection of threats and sometimes lets it's opponent make seemingly threatening moves (like getting 4 in a row) if it knows it can shut the threat down with one move and uses the move before to develop threats of it's own. The shortcomings of this model is that it completely fails to generalize to moves that are very out-of-distribution. For example if you start, you can win by just placing down 5 consecutive moves in the top or bottom row. Although the data was highly augmented (4 rotations, 2 symmetries, and a maximum amount of translations of the game cluster) and has learned to be mostly location invariant, it seems it is still overfit and doesn't know how to react to these "edge" moves. It also has trouble managing threats when the game length exceeds that of the training distribution. 

## Files:
1. gym.py - Trains the policy network using the REINFORCE algorithm.
2. environment.py - Environment class, used to control the MDP.
3. game_mechanics.py - Functions that help with board manipulation (board creation, making moves, checking if the game is over, etc.)
3. model_new.py - Model class, creates the network, trains it, evaluates it, and predicts actions.
4. GUI.py - Interface for viewing games, playing games PvP (policy data generation), and playing games PvAI (model evaluation)
5. data_functions.py - Functions that manipulate, augment, or load data.
